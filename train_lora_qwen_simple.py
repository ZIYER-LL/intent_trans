import os
import json
import torch
from datetime import datetime
from pathlib import Path

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    set_seed
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from trl import SFTTrainer
from datasets import Dataset

# =====================
# é…ç½®å‚æ•°
# =====================
MODEL_DIR = "/work/2024/zhulei/intent-driven/qwen3-4b"  # æ¨¡å‹è·¯å¾„
TRAIN_DATA_PATH = "/work/2024/zhulei/intent-driven/train_intent_qwen.json"  # è®­ç»ƒæ•°æ®è·¯å¾„
OUTPUT_DIR = "/work/2024/zhulei/intent-driven/outputs/qwen3-4b-lora-intent"  # è¾“å‡ºç›®å½•

# LoRAå‚æ•°
LORA_R = 8  # LoRA rank
LORA_ALPHA = 32  # LoRA alpha
LORA_DROPOUT = 0.1  # LoRA dropout
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # ç›®æ ‡æ¨¡å—

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 4  # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
GRADIENT_ACCUMULATION_STEPS = 4  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
LEARNING_RATE = 2e-4  # å­¦ä¹ ç‡
NUM_EPOCHS = 3  # è®­ç»ƒè½®æ•°
MAX_LENGTH = 1024  # æœ€å¤§åºåˆ—é•¿åº¦
SAVE_STEPS = 500  # æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡
LOGGING_STEPS = 50  # æ¯å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
WARMUP_STEPS = 100  # é¢„çƒ­æ­¥æ•°
FP16 = True  # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
GRADIENT_CHECKPOINTING = True  # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

# å…¶ä»–å‚æ•°
SEED = 42  # éšæœºç§å­
RESUME_FROM_CHECKPOINT = None  # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

# =====================
# å·¥å…·å‡½æ•°
# =====================

def load_dataset(data_path, tokenizer):
    """åŠ è½½æ•°æ®é›†å¹¶æ ¼å¼åŒ–ä¸ºæ–‡æœ¬ï¼ˆè®©SFTTrainerå¤„ç†tokenizationï¼‰"""
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {data_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"æ•°æ®é›†å¤§å°: {len(data)} æ¡")
    
    # éªŒè¯æ•°æ®æ ¼å¼
    if not isinstance(data, list):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šåº”è¯¥æ˜¯åˆ—è¡¨æ ¼å¼")
    
    if len(data) > 0 and "messages" not in data[0]:
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šæ¯ä¸ªæ¡ç›®åº”åŒ…å«'messages'å­—æ®µ")
    
    # ä½¿ç”¨tokenizerçš„apply_chat_templateæ ¼å¼åŒ–å¯¹è¯ä¸ºæ–‡æœ¬
    # SFTTrainerä¼šè‡ªåŠ¨å¤„ç†tokenizationï¼Œæ‰€ä»¥è¿™é‡Œåªæ ¼å¼åŒ–ï¼Œä¸tokenize
    formatted_data = []
    for idx, item in enumerate(data):
        try:
            if "messages" not in item:
                print(f"è­¦å‘Šï¼šç¬¬ {idx+1} æ¡æ•°æ®ç¼ºå°‘'messages'å­—æ®µï¼Œå·²è·³è¿‡")
                continue
            
            # ä½¿ç”¨apply_chat_templateå°†messagesæ ¼å¼åŒ–ä¸ºæ–‡æœ¬
            # tokenize=False è¡¨ç¤ºåªæ ¼å¼åŒ–ï¼Œä¸è¿›è¡Œtokenization
            text = tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
                add_generation_prompt=False
            )
            formatted_data.append({"text": text})
        except Exception as e:
            print(f"è­¦å‘Šï¼šå¤„ç†ç¬¬ {idx+1} æ¡æ•°æ®æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡")
            continue
    
    print(f"æˆåŠŸå¤„ç† {len(formatted_data)} æ¡æ•°æ®")
    
    # è½¬æ¢ä¸ºDatasetæ ¼å¼ï¼Œå­—æ®µåä¸º"text"
    # SFTTrainerä¼šè¯»å–è¿™ä¸ª"text"å­—æ®µå¹¶è¿›è¡Œtokenization
    dataset = Dataset.from_list(formatted_data)
    
    return dataset

# =====================
# ä¸»å‡½æ•°
# =====================

def main():
    # è®¾ç½®éšæœºç§å­
    set_seed(SEED)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU
    if torch.cuda.is_available():
        print(f"âœ… GPUå¯ç”¨ï¼è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
        # æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU (cuda:0)
        torch.cuda.set_device(0)
        print(f"\nğŸ¯ æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU: cuda:0 ({torch.cuda.get_device_name(0)})")
    else:
        print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        print("   å»ºè®®ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½tokenizer
    print(f"æ­£åœ¨åŠ è½½tokenizer: {MODEL_DIR}")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        trust_remote_code=True
    )
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_DIR}")
    # æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU (cuda:0)
    if torch.cuda.is_available():
        device = "cuda:0"
        print(f"æŒ‡å®šä½¿ç”¨GPU: {device} ({torch.cuda.get_device_name(0)})")
    else:
        device = "cpu"
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        torch_dtype=torch.float16 if FP16 else torch.float32,
        device_map=device,  # æ˜ç¡®æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPU
        trust_remote_code=True
    )
    
    # æ‰“å°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    print(f"æ¨¡å‹å·²åŠ è½½åˆ°: {next(model.parameters()).device}")
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
        print("å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # é…ç½®LoRA
    print("é…ç½®LoRAå‚æ•°...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_dataset = load_dataset(TRAIN_DATA_PATH, tokenizer)
    
    # è®­ç»ƒå‚æ•°
    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„transformersï¼š4.21.0+ä½¿ç”¨eval_strategyï¼Œæ—§ç‰ˆæœ¬ä½¿ç”¨evaluation_strategy
    training_args_dict = {
        "output_dir": str(output_dir),
        "overwrite_output_dir": True,
        "num_train_epochs": NUM_EPOCHS,
        "per_device_train_batch_size": BATCH_SIZE,
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
        "learning_rate": LEARNING_RATE,
        "fp16": FP16,
        "logging_steps": LOGGING_STEPS,
        "save_steps": SAVE_STEPS,
        "save_total_limit": 3,  # åªä¿ç•™æœ€è¿‘3ä¸ªæ£€æŸ¥ç‚¹
        "warmup_steps": WARMUP_STEPS,
        "report_to": "tensorboard" if os.path.exists("tensorboard") else None,
        "dataloader_pin_memory": True,
        "save_safetensors": True,
    }
    
    # æ ¹æ®transformersç‰ˆæœ¬é€‰æ‹©æ­£ç¡®çš„å‚æ•°å
    try:
        # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„å‚æ•°åï¼ˆ4.21.0+ï¼‰
        training_args = TrainingArguments(**training_args_dict, eval_strategy="no")
    except TypeError:
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ—§ç‰ˆæœ¬çš„å‚æ•°å
        training_args = TrainingArguments(**training_args_dict, evaluation_strategy="no")
    
    # åˆ›å»ºSFTTrainer
    # ä¸åŒç‰ˆæœ¬çš„trlåº“å¯èƒ½æœ‰ä¸åŒçš„å‚æ•°ï¼Œè¿™é‡Œä½¿ç”¨å…¼å®¹çš„æ–¹å¼
    # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„å‚æ•°ï¼ˆåŒ…å«tokenizerï¼‰
    try:
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            max_seq_length=MAX_LENGTH,
            dataset_text_field="text",
        )
    except TypeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨tokenizerå‚æ•°ï¼ˆæŸäº›ç‰ˆæœ¬ä¼šè‡ªåŠ¨ä»modelè·å–ï¼‰
        try:
            trainer = SFTTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                max_seq_length=MAX_LENGTH,
                dataset_text_field="text",
            )
        except TypeError:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨æœ€ç®€å‚æ•°
            trainer = SFTTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
            )
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if RESUME_FROM_CHECKPOINT:
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {RESUME_FROM_CHECKPOINT}")
        trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)
    else:
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒ...")
        trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(str(output_dir))
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_to_save = {
        "model_dir": MODEL_DIR,
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
        "lora_dropout": LORA_DROPOUT,
        "target_modules": TARGET_MODULES,
        "max_length": MAX_LENGTH,
        "training_args": training_args.to_dict(),
        "train_time": datetime.now().isoformat()
    }
    
    with open(output_dir / "training_config.json", "w", encoding="utf-8") as f:
        json.dump(config_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    print(f"LoRAæƒé‡ä¿å­˜åœ¨: {output_dir / 'adapter_model.bin'}")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print(f"  from peft import PeftModel")
    print(f"  model = AutoModelForCausalLM.from_pretrained('{MODEL_DIR}')")
    print(f"  model = PeftModel.from_pretrained(model, '{output_dir}')")

if __name__ == "__main__":
    main()

