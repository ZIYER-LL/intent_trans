#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import torch
from datetime import datetime
from pathlib import Path
import time
import math
import re
from collections import defaultdict

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    set_seed
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from trl import SFTTrainer
from datasets import Dataset

# =====================
# é…ç½®å‚æ•°
# =====================
MODEL_DIR = "/work/2024/zhulei/intent-driven/qwen3-4b"  # æ¨¡å‹è·¯å¾„
TRAIN_DATA_PATH = "/work/2024/zhulei/intent-driven/train_qwen3.jsonl"  # è®­ç»ƒæ•°æ®è·¯å¾„
OUTPUT_DIR = "/work/2024/zhulei/intent-driven/outputs/qwen3-4b-lora"  # è¾“å‡ºç›®å½•

# LoRAå‚æ•°
LORA_R = 8  # LoRA rank
LORA_ALPHA = 32  # LoRA alpha
LORA_DROPOUT = 0.1  # LoRA dropout
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # ç›®æ ‡æ¨¡å—

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 4  # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
GRADIENT_ACCUMULATION_STEPS = 4  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
LEARNING_RATE = 2e-4  # å­¦ä¹ ç‡
NUM_EPOCHS = 3  # è®­ç»ƒè½®æ•°
MAX_LENGTH = 1024  # æœ€å¤§åºåˆ—é•¿åº¦
SAVE_STEPS = 500  # æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡
LOGGING_STEPS = 50  # æ¯å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
WARMUP_STEPS = 100  # é¢„çƒ­æ­¥æ•°
FP16 = True  # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
GRADIENT_CHECKPOINTING = True  # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

# å…¶ä»–å‚æ•°
SEED = 42  # éšæœºç§å­
RESUME_FROM_CHECKPOINT = None  # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
GPU_ID = 2  # æŒ‡å®šä½¿ç”¨çš„GPU IDï¼ˆæ ¹æ®nvidia-smié€‰æ‹©ç©ºé—²çš„GPUï¼ŒGPU 2/4/5/6/7éƒ½å¯ç”¨ï¼‰

# =====================
# å·¥å…·å‡½æ•°
# =====================

def load_dataset(data_path, tokenizer):
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {data_path}")

    if not os.path.exists(data_path):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

    data_path_lower = data_path.lower()
    data = []

    # âœ… å…¼å®¹ jsonlï¼šä¸€è¡Œä¸€ä¸ª JSON
    if data_path_lower.endswith(".jsonl"):
        with open(data_path, "r", encoding="utf-8") as f:
            for line_no, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    raise ValueError(f"ç¬¬ {line_no} è¡Œä¸æ˜¯åˆæ³• JSONï¼š{e}") from e
    else:
        with open(data_path, "r", encoding="utf-8") as f:
            data = json.load(f)

    print(f"æ•°æ®é›†å¤§å°: {len(data)} æ¡")

    if not isinstance(data, list):
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šåº”è¯¥æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆæˆ– jsonl æ¯è¡Œä¸€ä¸ªå¯¹è±¡ï¼‰")

    if len(data) > 0 and "messages" not in data[0]:
        raise ValueError("æ•°æ®æ ¼å¼é”™è¯¯ï¼šæ¯ä¸ªæ¡ç›®åº”åŒ…å« 'messages' å­—æ®µ")

    formatted_data = []
    for idx, item in enumerate(data):
        try:
            text = tokenizer.apply_chat_template(
                item["messages"],
                tokenize=False,
                add_generation_prompt=False
            )
            formatted_data.append({"text": text})
        except Exception as e:
            print(f"è­¦å‘Šï¼šå¤„ç†ç¬¬ {idx+1} æ¡æ•°æ®æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡")
            continue

    print(f"æˆåŠŸå¤„ç† {len(formatted_data)} æ¡æ•°æ®")
    return Dataset.from_list(formatted_data)

def _grad_norm(model, only_lora: bool = False) -> float:
    total_sq = 0.0
    for name, p in model.named_parameters():
        if p.grad is None:
            continue
        if only_lora and ("lora" not in name.lower()):
            continue
        g = p.grad.detach()
        if g.is_sparse:
            g = g.coalesce().values()
        gn = g.float().norm(2).item()
        total_sq += gn * gn
    return math.sqrt(total_sq)


def _lora_param_norms_by_group(model, top_k: int = 12):
    """
    ç»Ÿè®¡ LoRA å‚æ•°èŒƒæ•°ï¼ŒæŒ‰ layer_id + module_type åˆ†ç»„å– TopKï¼Œ
    é¿å… TensorBoard æ›²çº¿å¤ªå¤šã€‚
    """
    groups_sq = defaultdict(float)

    # å…¼å®¹å¸¸è§ç»“æ„ï¼šmodel.layers.N æˆ– transformer.h.N
    layer_pat = re.compile(r"(?:layers|h)\.(\d+)")
    module_keys = ["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj",
                   "w1", "w2", "w3", "fc1", "fc2"]

    for name, p in model.named_parameters():
        lname = name.lower()
        if "lora" not in lname:
            continue

        m = layer_pat.search(name)
        layer_id = m.group(1) if m else "misc"

        mod = "misc"
        for k in module_keys:
            if k in lname:
                mod = k
                break

        gname = f"layer{layer_id}/{mod}"
        pn = p.detach().float().norm(2).item()
        groups_sq[gname] += pn * pn

    groups = {k: math.sqrt(v) for k, v in groups_sq.items()}
    top = dict(sorted(groups.items(), key=lambda x: x[1], reverse=True)[:top_k])
    return top


class LoRAMonitorSFTTrainer(SFTTrainer):
    """
    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†™ TensorBoard æ ‡é‡ï¼š
    - train loss + loss_ema
    - lr
    - grad_norm_all / grad_norm_lora
    - lora_param_norm (TopK grouped)
    - tokens/sec
    - step_time + gpu_mem
    """
    def __init__(self, *args, ema_beta: float = 0.98, lora_top_k: int = 12, **kwargs):
        super().__init__(*args, **kwargs)
        self.ema_beta = ema_beta
        self.loss_ema = None
        self.lora_top_k = lora_top_k

    def training_step(self, model, inputs, num_items_in_batch=None):
        model.train()
        inputs = self._prepare_inputs(inputs)
    
        step_start = time.perf_counter()
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
    
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)
    
        if self.args.n_gpu > 1:
            loss = loss.mean()
    
        self.accelerator.backward(loss)
    
        if self.is_world_process_zero() and (self.state.global_step % self.args.logging_steps == 0):
            loss_val = float(loss.detach().float().item())
            if self.loss_ema is None:
                self.loss_ema = loss_val
            else:
                b = self.ema_beta
                self.loss_ema = b * self.loss_ema + (1 - b) * loss_val
    
            lr = 0.0
            if self.optimizer is not None and len(self.optimizer.param_groups) > 0:
                lr = float(self.optimizer.param_groups[0].get("lr", 0.0))
    
            gn_all = _grad_norm(model, only_lora=False)
            gn_lora = _grad_norm(model, only_lora=True)
    
            if "attention_mask" in inputs:
                tokens = int(inputs["attention_mask"].detach().sum().item())
            else:
                tokens = int(inputs["input_ids"].detach().numel())
    
            step_time = time.perf_counter() - step_start
            tps = tokens / max(step_time, 1e-8)
    
            mem_gb = 0.0
            if torch.cuda.is_available():
                mem_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)
    
            logs = {
                "train/loss": loss_val,
                "train/loss_ema": float(self.loss_ema),
                "train/lr": lr,
                "train/grad_norm_all": float(gn_all),
                "train/grad_norm_lora": float(gn_lora),
                "train/tokens_per_sec": float(tps),
                "train/step_time_sec": float(step_time),
                "train/gpu_mem_gb": float(mem_gb),
            }
    
            for k, v in _lora_param_norms_by_group(model, top_k=self.lora_top_k).items():
                logs[f"train/lora_param_norm/{k}"] = float(v)
    
            self.log(logs)
    
        return loss.detach() / self.args.gradient_accumulation_steps

# =====================
# ä¸»å‡½æ•°
# =====================

def main():
    # è®¾ç½®éšæœºç§å­
    set_seed(SEED)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶æŒ‡å®šä½¿ç”¨æŒ‡å®šçš„GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… GPUå¯ç”¨ï¼è®¾å¤‡æ•°é‡: {gpu_count}")
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_total = props.total_memory / 1024**3
            print(f"  GPU {i}: {props.name}")
            print(f"    æ€»æ˜¾å­˜: {memory_total:.2f} GB")
            if i == GPU_ID:
                print(f"    â­ å·²é€‰æ‹©æ­¤GPU")
        
        # æ£€æŸ¥æŒ‡å®šçš„GPU IDæ˜¯å¦æœ‰æ•ˆ
        if GPU_ID >= gpu_count:
            print(f"âš ï¸  è­¦å‘Šï¼šæŒ‡å®šçš„GPU {GPU_ID}ä¸å­˜åœ¨ï¼Œåªæœ‰{gpu_count}å—GPUï¼Œå°†ä½¿ç”¨GPU 0")
            selected_gpu = 0
        else:
            selected_gpu = GPU_ID
        
        # æŒ‡å®šä½¿ç”¨é€‰å®šçš„GPU
        torch.cuda.set_device(selected_gpu)
        print(f"\nğŸ¯ æŒ‡å®šä½¿ç”¨GPU {selected_gpu}: cuda:{selected_gpu} ({torch.cuda.get_device_name(selected_gpu)})")
        sys.stdout.flush()
    else:
        print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        print("   å»ºè®®ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ")
        selected_gpu = None
        sys.stdout.flush()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½tokenizer
    print(f"æ­£åœ¨åŠ è½½tokenizer: {MODEL_DIR}")
    sys.stdout.flush()
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        trust_remote_code=True
    )
    print("âœ… TokenizeråŠ è½½å®Œæˆ")
    sys.stdout.flush()
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_DIR}")
    print("âš ï¸  æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    sys.stdout.flush()
    
    # æŒ‡å®šä½¿ç”¨é€‰å®šçš„GPU
    if torch.cuda.is_available() and selected_gpu is not None:
        device = f"cuda:{selected_gpu}"
        print(f"æŒ‡å®šä½¿ç”¨GPU: {device} ({torch.cuda.get_device_name(selected_gpu)})")
        sys.stdout.flush()
        print("å¼€å§‹åŠ è½½æ¨¡å‹æƒé‡...")
        sys.stdout.flush()
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.float16 if FP16 else torch.float32,
            device_map={"": device},  # ä½¿ç”¨å­—å…¸æ ¼å¼æŒ‡å®šè®¾å¤‡ï¼Œä¿®å¤device_mapå‚æ•°é—®é¢˜
            trust_remote_code=True
        )
    else:
        device = "cpu"
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
        sys.stdout.flush()
        print("å¼€å§‹åŠ è½½æ¨¡å‹æƒé‡...")
        sys.stdout.flush()
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.float32,  # CPUä¸æ”¯æŒfloat16
            device_map="cpu",
            trust_remote_code=True
        )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    # æ‰“å°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    print(f"æ¨¡å‹å·²åŠ è½½åˆ°: {next(model.parameters()).device}")
    sys.stdout.flush()
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
        model.enable_input_require_grads()
        print("å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ + enable_input_require_grads")
    
    # é…ç½®LoRA
    print("é…ç½®LoRAå‚æ•°...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias="none",
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("\nå¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...")
    sys.stdout.flush()
    train_dataset = load_dataset(TRAIN_DATA_PATH, tokenizer)
    print("âœ… è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ\n")
    sys.stdout.flush()
    
    # è®­ç»ƒå‚æ•°
    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„transformersï¼š4.21.0+ä½¿ç”¨eval_strategyï¼Œæ—§ç‰ˆæœ¬ä½¿ç”¨evaluation_strategy
    training_args_dict = {
        "output_dir": str(output_dir),
        "overwrite_output_dir": True,
        "num_train_epochs": NUM_EPOCHS,
        "per_device_train_batch_size": BATCH_SIZE,
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
        "learning_rate": LEARNING_RATE,
        "fp16": FP16,
        "logging_steps": LOGGING_STEPS,
        "logging_strategy": "steps",
        "save_steps": SAVE_STEPS,
        "save_total_limit": 3,
        "warmup_steps": WARMUP_STEPS,
    
        # âœ… TensorBoard
        "report_to": ["tensorboard"],
        "logging_dir": str(output_dir / "tb"),
    
        "dataloader_pin_memory": True,
        "save_safetensors": True,
    }
    
    # æ ¹æ®transformersç‰ˆæœ¬é€‰æ‹©æ­£ç¡®çš„å‚æ•°å
    try:
        # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„å‚æ•°åï¼ˆ4.21.0+ï¼‰
        training_args = TrainingArguments(**training_args_dict, eval_strategy="no")
    except TypeError:
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ—§ç‰ˆæœ¬çš„å‚æ•°å
        training_args = TrainingArguments(**training_args_dict, evaluation_strategy="no")
    
    # åˆ›å»ºSFTTrainer
    # ä¸åŒç‰ˆæœ¬çš„trlåº“å¯èƒ½æœ‰ä¸åŒçš„å‚æ•°ï¼Œè¿™é‡Œä½¿ç”¨å…¼å®¹çš„æ–¹å¼
    # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„å‚æ•°ï¼ˆåŒ…å«tokenizerï¼‰
    try:
        trainer = LoRAMonitorSFTTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            max_seq_length=MAX_LENGTH,
            dataset_text_field="text",
        )
    except TypeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨tokenizerå‚æ•°ï¼ˆæŸäº›ç‰ˆæœ¬ä¼šè‡ªåŠ¨ä»modelè·å–ï¼‰
        try:
            trainer = LoRAMonitorSFTTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                max_seq_length=MAX_LENGTH,
                dataset_text_field="text",
            )
        except TypeError:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨æœ€ç®€å‚æ•°
            trainer = LoRAMonitorSFTTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
            )
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if RESUME_FROM_CHECKPOINT:
        print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {RESUME_FROM_CHECKPOINT}")
        trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)
    else:
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒ...")
        trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(str(output_dir))
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_to_save = {
        "model_dir": MODEL_DIR,
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
        "lora_dropout": LORA_DROPOUT,
        "target_modules": TARGET_MODULES,
        "max_length": MAX_LENGTH,
        "training_args": training_args.to_dict(),
        "train_time": datetime.now().isoformat()
    }
    
    with open(output_dir / "training_config.json", "w", encoding="utf-8") as f:
        json.dump(config_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    print(f"LoRAæƒé‡ä¿å­˜åœ¨: {output_dir / 'adapter_model.bin'}")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print(f"  from peft import PeftModel")
    print(f"  model = AutoModelForCausalLM.from_pretrained('{MODEL_DIR}')")
    print(f"  model = PeftModel.from_pretrained(model, '{output_dir}')")

if __name__ == "__main__":
    main()










