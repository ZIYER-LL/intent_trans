from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")

try:
    model_name = "THUDM/chatglm-6b"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
    model = model.half().cuda()
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    prompt = "å¸®æˆ‘å†™ä¸€æ¡æé†’ï¼šæ˜å¤©ä¸Šåˆä¹ç‚¹å¼€ä¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=100)
    print("è¾“å‡ºï¼š", tokenizer.decode(output[0], skip_special_tokens=True))

except Exception as e:
    print("âŒ å‡ºé”™äº†ï¼š", e)