#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºç¡€æ¨¡å‹æµ‹è¯•è„šæœ¬
è®¡ç®— intent_type accuracy, service_type accuracy, joint accuracy
ä¸åŠ è½½LoRAæƒé‡ï¼Œä»…æµ‹è¯•åŸºç¡€æ¨¡å‹æ€§èƒ½
"""

import os
import json
import re
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

# =====================
# é…ç½®å‚æ•°
# =====================
BASE_MODEL_DIR = "/work/2024/zhulei/intent-driven/qwen3-4b"  # åŸºç¡€æ¨¡å‹è·¯å¾„
TEST_DATA_PATH = "/work/2024/zhulei/intent-driven/test_intent.json"  # æµ‹è¯•æ•°æ®è·¯å¾„
GPU_ID = 2  # ä½¿ç”¨çš„GPU ID

# æ¨ç†å‚æ•°
MAX_NEW_TOKENS = 100  # å‡å°‘tokenæ•°é‡ï¼Œåªéœ€è¦JSONè¾“å‡º
DO_SAMPLE = False  # ä½¿ç”¨è´ªå©ªè§£ç ï¼Œæ›´ç¡®å®š

# =====================
# å·¥å…·å‡½æ•°
# =====================

def load_base_model(base_model_dir, gpu_id=0):
    """åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¸åŠ è½½LoRAæƒé‡ï¼‰"""
    print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_dir}")
    
    if torch.cuda.is_available():
        device = f"cuda:{gpu_id}"
        torch.cuda.set_device(gpu_id)
        dtype = torch.float16
        device_map = {"": device}
    else:
        device = "cpu"
        dtype = torch.float32
        device_map = "cpu"
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_dir,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("åŠ è½½åŸºç¡€æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_dir,
        torch_dtype=dtype,
        device_map=device_map,
        trust_remote_code=True
    )
    
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    return tokenizer, model

def create_prompt(input_text):
    """æ ¹æ®è¾“å…¥æ–‡æœ¬æ„é€ prompt"""
    # ä½¿ç”¨è¯¦ç»†çš„æ–‡æœ¬promptï¼Œæ˜ç¡®åˆ—å‡ºæ‰€æœ‰å¯é€‰å€¼ï¼Œè¦æ±‚åªè¾“å‡ºJSON
    prompt = f"""ä»»åŠ¡ï¼šä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«intent_typeå’Œservice_typeã€‚

å¯é€‰å€¼ï¼š
intent_type: slice_create, slice_qos_modify, route_preference, access_control
service_type: realtime_video, realtime_voice_call, realtime_xr_gaming, streaming_video, streaming_live, file_transfer, iot_sensor, internet_access, urllc_control

ç”¨æˆ·è¾“å…¥ï¼š{input_text}

åªè¾“å‡ºJSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ï¼š"""
    return prompt

def parse_model_output(output_text, input_text):
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­è§£æintent_typeå’Œservice_type
    é‡‡ç”¨å¤šçº§fallbackç­–ç•¥ï¼šå…ˆå°è¯•ç›´æ¥è§£æJSONï¼Œå†æå–JSONå¯¹è±¡ï¼Œæœ€åä½¿ç”¨æ­£åˆ™æå–å­—æ®µ
    """
    intent_type = None
    service_type = None
    
    # å·²çŸ¥çš„æ‰€æœ‰å¯èƒ½å€¼
    known_intents = ["slice_create", "route_preference", "slice_qos_modify", "access_control"]
    known_services = [
        "realtime_video", "realtime_voice_call", "realtime_xr_gaming",
        "streaming_live", "streaming_video", "iot_sensor", 
        "urllc_control", "internet_access"
    ]
    
    # æ–¹æ³•1: å°è¯•ç›´æ¥è§£æJSON
    try:
        parsed = json.loads(output_text)
        intent_type = parsed.get("intent_type")
        service_type = parsed.get("service_type")
        if intent_type and service_type:
            return intent_type, service_type
    except:
        pass
    
    # æ–¹æ³•2: å°è¯•æå–ç¬¬ä¸€ä¸ª { ... } ä¹‹é—´çš„å†…å®¹
    try:
        start = output_text.find("{")
        end = output_text.rfind("}") + 1
        if start >= 0 and end > start:
            json_str = output_text[start:end]
            parsed = json.loads(json_str)
            intent_type = parsed.get("intent_type")
            service_type = parsed.get("service_type")
            if intent_type and service_type:
                return intent_type, service_type
    except:
        pass
    
    # æ–¹æ³•3: ä½¿ç”¨æ­£åˆ™æå–JSONå­—æ®µ
    intent_json_pattern = r'"intent_type"\s*:\s*"([^"]+)"'
    intent_match = re.search(intent_json_pattern, output_text, re.IGNORECASE)
    if intent_match:
        intent_type = intent_match.group(1).strip()
    
    service_json_pattern = r'"service_type"\s*:\s*"([^"]+)"'
    service_match = re.search(service_json_pattern, output_text, re.IGNORECASE)
    if service_match:
        service_type = service_match.group(1).strip()
    
    # æ–¹æ³•4: éªŒè¯æå–çš„å€¼æ˜¯å¦åœ¨å·²çŸ¥åˆ—è¡¨ä¸­
    if intent_type and intent_type not in known_intents:
        intent_type = None
    if service_type and service_type not in known_services:
        service_type = None
    
    return intent_type, service_type

def infer(model, tokenizer, input_text):
    """å¯¹è¾“å…¥è¿›è¡Œæ¨ç†"""
    # æ„é€ æ–‡æœ¬promptï¼ˆä¸ä½¿ç”¨messagesæ ¼å¼ï¼‰
    prompt_text = create_prompt(input_text)
    
    # Tokenize
    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)
    
    # æ¨ç† - ä½¿ç”¨è´ªå©ªè§£ç ï¼ˆdo_sample=Falseï¼‰ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„è¾“å‡º
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,  # å‡å°‘tokenæ•°é‡ï¼Œåªéœ€è¦JSONè¾“å‡º
            do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç ï¼Œæ›´ç¡®å®š
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç å®Œæ•´è¾“å‡ºï¼ˆåŒ…å«è¾“å…¥å’Œç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥promptï¼‰
    # ç®€å•æ–¹æ³•ï¼šæ‰¾åˆ°promptçš„ç»“å°¾ï¼Œå–åé¢çš„éƒ¨åˆ†
    if prompt_text in full_output:
        output_text = full_output.split(prompt_text, 1)[1]
    else:
        # å¦‚æœæ‰¾ä¸åˆ°promptï¼Œå–æ–°ç”Ÿæˆçš„tokens
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # ç®€åŒ–æˆªæ–­ï¼šæå–ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
    start = output_text.find("{")
    end = output_text.rfind("}")
    if start >= 0 and end > start:
        output_text = output_text[start:end+1]
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œæˆªæ–­åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œæˆ–åˆç†é•¿åº¦
        for stop_char in ["\n\n", "\n", "ã€‚", "ï¼Œ"]:
            if stop_char in output_text:
                output_text = output_text.split(stop_char)[0]
                break
        if len(output_text) > 200:
            output_text = output_text[:200]
    
    return output_text.strip()

def load_test_data(test_data_path):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
    with open(test_data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"æµ‹è¯•é›†å¤§å°: {len(data)} æ¡")
    return data

def calculate_metrics(results):
    """è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡"""
    total = len(results)
    intent_correct = 0
    service_correct = 0
    joint_correct = 0
    
    for result in results:
        if result['intent_pred'] == result['intent_true']:
            intent_correct += 1
        if result['service_pred'] == result['service_true']:
            service_correct += 1
        if (result['intent_pred'] == result['intent_true'] and 
            result['service_pred'] == result['service_true']):
            joint_correct += 1
    
    intent_acc = intent_correct / total if total > 0 else 0
    service_acc = service_correct / total if total > 0 else 0
    joint_acc = joint_correct / total if total > 0 else 0
    
    return {
        'intent_accuracy': intent_acc,
        'service_accuracy': service_acc,
        'joint_accuracy': joint_acc,
        'intent_correct': intent_correct,
        'service_correct': service_correct,
        'joint_correct': joint_correct,
        'total': total
    }

def main():
    print("=" * 60)
    print("ğŸš€ åŸºç¡€æ¨¡å‹æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(BASE_MODEL_DIR):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {BASE_MODEL_DIR}")
    
    if not os.path.exists(TEST_DATA_PATH):
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {TEST_DATA_PATH}")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_base_model(BASE_MODEL_DIR, GPU_ID)
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}\n")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(TEST_DATA_PATH)
    
    # è¿›è¡Œæµ‹è¯•
    print("å¼€å§‹æµ‹è¯•...")
    print("-" * 60)
    
    results = []
    for idx, item in enumerate(test_data, 1):
        input_text = item['input']
        intent_true = item['intent_type']
        service_true = item['service_type']
        
        # æ¨ç†
        try:
            output_text = infer(model, tokenizer, input_text)
            intent_pred, service_pred = parse_model_output(output_text, input_text)
        except Exception as e:
            print(f"âš ï¸  ç¬¬ {idx} æ¡æµ‹è¯•å‡ºé”™: {e}")
            intent_pred = None
            service_pred = None
            output_text = ""
        
        # è®°å½•ç»“æœ
        result = {
            'idx': idx,
            'input': input_text,
            'intent_true': intent_true,
            'intent_pred': intent_pred,
            'service_true': service_true,
            'service_pred': service_pred,
            'output': output_text,
            'intent_correct': intent_pred == intent_true,
            'service_correct': service_pred == service_true,
            'joint_correct': (intent_pred == intent_true and service_pred == service_true)
        }
        results.append(result)
        
        # æ˜¾ç¤ºè¿›åº¦
        if idx % 10 == 0:
            print(f"å·²æµ‹è¯• {idx}/{len(test_data)} æ¡...")
    
    print("-" * 60)
    print("æµ‹è¯•å®Œæˆï¼\n")
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(results)
    
    # æ˜¾ç¤ºç»“æœ
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœï¼ˆåŸºç¡€æ¨¡å‹ï¼‰")
    print("=" * 60)
    print(f"æ€»æµ‹è¯•æ ·æœ¬æ•°: {metrics['total']}")
    print(f"\nIntent Type Accuracy: {metrics['intent_accuracy']:.4f} ({metrics['intent_correct']}/{metrics['total']})")
    print(f"Service Type Accuracy: {metrics['service_accuracy']:.4f} ({metrics['service_correct']}/{metrics['total']})")
    print(f"Joint Accuracy: {metrics['joint_accuracy']:.4f} ({metrics['joint_correct']}/{metrics['total']})")
    print("=" * 60)
    
    # æ˜¾ç¤ºé”™è¯¯æ ·æœ¬
    print("\nâŒ é”™è¯¯æ ·æœ¬åˆ†æ:")
    print("-" * 60)
    
    intent_errors = [r for r in results if not r['intent_correct']]
    service_errors = [r for r in results if not r['service_correct']]
    joint_errors = [r for r in results if not r['joint_correct']]
    
    print(f"\nIntenté”™è¯¯ ({len(intent_errors)} ä¸ª):")
    for r in intent_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  è¾“å…¥: {r['input'][:50]}...")
        print(f"  çœŸå®: {r['intent_true']}, é¢„æµ‹: {r['intent_pred']}")
        print(f"  è¾“å‡º: {r['output'][:100]}...")
        print()
    
    print(f"\nServiceé”™è¯¯ ({len(service_errors)} ä¸ª):")
    for r in service_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  è¾“å…¥: {r['input'][:50]}...")
        print(f"  çœŸå®: {r['service_true']}, é¢„æµ‹: {r['service_pred']}")
        print(f"  è¾“å‡º: {r['output'][:100]}...")
        print()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "test_base_model_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'model_type': 'base_model',
            'base_model_dir': BASE_MODEL_DIR,
            'metrics': metrics,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()










