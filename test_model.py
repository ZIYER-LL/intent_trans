#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æµ‹è¯•è„šæœ¬
è®¡ç®— intent_type accuracy, service_type accuracy, joint accuracy
"""

import os
import json
import re
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# =====================
# é…ç½®å‚æ•°
# =====================
BASE_MODEL_DIR = "/work/2024/zhulei/intent-driven/qwen3-4b"  # åŸºç¡€æ¨¡å‹è·¯å¾„
LORA_MODEL_DIR = "/work/2024/zhulei/intent-driven/outputs/qwen3-4b-lora-intent"  # LoRAæ¨¡å‹è·¯å¾„
TEST_DATA_PATH = "/work/2024/zhulei/intent-driven/test_intent.json"  # æµ‹è¯•æ•°æ®è·¯å¾„ï¼ˆç›¸å¯¹äºè„šæœ¬è¿è¡Œç›®å½•ï¼‰
GPU_ID = 2  # ä½¿ç”¨çš„GPU ID

# æ¨ç†å‚æ•°
MAX_NEW_TOKENS = 256
TEMPERATURE = 0.1  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„è¾“å‡º
TOP_P = 0.9
DO_SAMPLE = True

# =====================
# å·¥å…·å‡½æ•°
# =====================

def load_model_with_lora(base_model_dir, lora_model_dir, gpu_id=0):
    """åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAæƒé‡"""
    print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_dir}")
    
    if torch.cuda.is_available():
        device = f"cuda:{gpu_id}"
        torch.cuda.set_device(gpu_id)
        dtype = torch.float16
        device_map = {"": device}
    else:
        device = "cpu"
        dtype = torch.float32
        device_map = "cpu"
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_dir,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_dir,
        torch_dtype=dtype,
        device_map=device_map,
        trust_remote_code=True
    )
    
    # åŠ è½½LoRAæƒé‡
    if os.path.exists(lora_model_dir):
        print(f"åŠ è½½LoRAæƒé‡: {lora_model_dir}")
        model = PeftModel.from_pretrained(base_model, lora_model_dir)
        print("âœ… LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
    else:
        print(f"âš ï¸  LoRAè·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {lora_model_dir}")
        model = base_model
    
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    return tokenizer, model

def create_prompt(input_text):
    """æ ¹æ®è¾“å…¥æ–‡æœ¬æ„é€ promptï¼ˆä½¿ç”¨messagesæ ¼å¼ï¼‰"""
    messages = [
        {
            "role": "user",
            "content": input_text
        }
    ]
    return messages

def parse_model_output(output_text, input_text):
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­è§£æintent_typeå’Œservice_type
    æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼š
    1. JSONæ ¼å¼: {"intent_type": "...", "service_type": "..."}
    2. è‡ªç„¶è¯­è¨€æ ¼å¼: intent_type: xxx, service_type: xxx
    3. å…¶ä»–æ ¼å¼
    """
    intent_type = None
    service_type = None
    
    # å·²çŸ¥çš„æ‰€æœ‰å¯èƒ½å€¼
    known_intents = ["slice_create", "route_preference", "slice_qos_modify", "access_control"]
    known_services = [
        "realtime_video", "realtime_voice_call", "realtime_xr_gaming",
        "streaming_live", "streaming_video", "iot_sensor", 
        "urllc_control", "internet_access"
    ]
    
    # æ–¹æ³•1: å°è¯•æå–å®Œæ•´çš„JSONæ ¼å¼
    # åŒ¹é… { ... "intent_type": "xxx" ... "service_type": "xxx" ... }
    json_patterns = [
        r'\{[^{}]*"intent_type"\s*:\s*"([^"]+)"[^{}]*"service_type"\s*:\s*"([^"]+)"[^{}]*\}',
        r'\{[^{}]*"service_type"\s*:\s*"([^"]+)"[^{}]*"intent_type"\s*:\s*"([^"]+)"[^{}]*\}',
    ]
    
    for pattern in json_patterns:
        json_match = re.search(pattern, output_text, re.IGNORECASE | re.DOTALL)
        if json_match:
            if "intent_type" in pattern:
                intent_type = json_match.group(1).strip()
                service_type = json_match.group(2).strip()
            else:
                service_type = json_match.group(1).strip()
                intent_type = json_match.group(2).strip()
            break
    
    # æ–¹æ³•2: å°è¯•æå–å•ç‹¬çš„JSONå­—æ®µ
    if intent_type is None:
        intent_json_pattern = r'"intent_type"\s*:\s*"([^"]+)"'
        intent_match = re.search(intent_json_pattern, output_text, re.IGNORECASE)
        if intent_match:
            intent_type = intent_match.group(1).strip()
    
    if service_type is None:
        service_json_pattern = r'"service_type"\s*:\s*"([^"]+)"'
        service_match = re.search(service_json_pattern, output_text, re.IGNORECASE)
        if service_match:
            service_type = service_match.group(1).strip()
    
    # æ–¹æ³•3: å°è¯•æå–é”®å€¼å¯¹æ ¼å¼ (intent_type: xxx æˆ– intent_type=xxx)
    if intent_type is None:
        intent_patterns = [
            r'intent_type["\s:ï¼š=]+\s*([a-z_]+)',
            r'intent["\s:ï¼š=]+\s*([a-z_]+)',
        ]
        for pattern in intent_patterns:
            intent_match = re.search(pattern, output_text, re.IGNORECASE)
            if intent_match:
                candidate = intent_match.group(1).strip()
                if candidate in known_intents:
                    intent_type = candidate
                    break
    
    if service_type is None:
        service_patterns = [
            r'service_type["\s:ï¼š=]+\s*([a-z_]+)',
            r'service["\s:ï¼š=]+\s*([a-z_]+)',
        ]
        for pattern in service_patterns:
            service_match = re.search(pattern, output_text, re.IGNORECASE)
            if service_match:
                candidate = service_match.group(1).strip()
                if candidate in known_services:
                    service_type = candidate
                    break
    
    # æ–¹æ³•4: åœ¨æ•´ä¸ªè¾“å‡ºä¸­æœç´¢å·²çŸ¥çš„å€¼ï¼ˆä½œä¸ºæœ€åæ‰‹æ®µï¼‰
    if intent_type is None:
        for intent in known_intents:
            # ä½¿ç”¨å•è¯è¾¹ç•Œç¡®ä¿å®Œæ•´åŒ¹é…
            pattern = r'\b' + re.escape(intent) + r'\b'
            if re.search(pattern, output_text, re.IGNORECASE):
                intent_type = intent
                break
    
    if service_type is None:
        for service in known_services:
            # ä½¿ç”¨å•è¯è¾¹ç•Œç¡®ä¿å®Œæ•´åŒ¹é…
            pattern = r'\b' + re.escape(service) + r'\b'
            if re.search(pattern, output_text, re.IGNORECASE):
                service_type = service
                break
    
    return intent_type, service_type

def infer(model, tokenizer, input_text):
    """å¯¹è¾“å…¥è¿›è¡Œæ¨ç†"""
    # æ„é€ messages
    messages = create_prompt(input_text)
    
    # ä½¿ç”¨tokenizerçš„apply_chat_templateæ ¼å¼åŒ–
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # æ¨ç†
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=DO_SAMPLE,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç è¾“å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    input_length = inputs['input_ids'].shape[1]
    generated_tokens = outputs[0][input_length:]
    output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    return output_text

def load_test_data(test_data_path):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
    with open(test_data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"æµ‹è¯•é›†å¤§å°: {len(data)} æ¡")
    return data

def calculate_metrics(results):
    """è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡"""
    total = len(results)
    intent_correct = 0
    service_correct = 0
    joint_correct = 0
    
    for result in results:
        if result['intent_pred'] == result['intent_true']:
            intent_correct += 1
        if result['service_pred'] == result['service_true']:
            service_correct += 1
        if (result['intent_pred'] == result['intent_true'] and 
            result['service_pred'] == result['service_true']):
            joint_correct += 1
    
    intent_acc = intent_correct / total if total > 0 else 0
    service_acc = service_correct / total if total > 0 else 0
    joint_acc = joint_correct / total if total > 0 else 0
    
    return {
        'intent_accuracy': intent_acc,
        'service_accuracy': service_acc,
        'joint_accuracy': joint_acc,
        'intent_correct': intent_correct,
        'service_correct': service_correct,
        'joint_correct': joint_correct,
        'total': total
    }

def main():
    print("=" * 60)
    print("ğŸš€ æ¨¡å‹æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(BASE_MODEL_DIR):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {BASE_MODEL_DIR}")
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æµ‹è¯•æ•°æ®è·¯å¾„
    test_paths = [
        TEST_DATA_PATH,
        os.path.join(os.path.dirname(os.path.dirname(__file__)), TEST_DATA_PATH),
        os.path.join(os.getcwd(), TEST_DATA_PATH),
    ]
    test_data_path = None
    for path in test_paths:
        if os.path.exists(path):
            test_data_path = path
            break
    
    if test_data_path is None:
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„è·¯å¾„: {test_paths}")
    
    TEST_DATA_PATH = test_data_path
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_model_with_lora(BASE_MODEL_DIR, LORA_MODEL_DIR, GPU_ID)
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}\n")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(test_data_path)
    
    # è¿›è¡Œæµ‹è¯•
    print("å¼€å§‹æµ‹è¯•...")
    print("-" * 60)
    
    results = []
    for idx, item in enumerate(test_data, 1):
        input_text = item['input']
        intent_true = item['intent_type']
        service_true = item['service_type']
        
        # æ¨ç†
        try:
            output_text = infer(model, tokenizer, input_text)
            intent_pred, service_pred = parse_model_output(output_text, input_text)
        except Exception as e:
            print(f"âš ï¸  ç¬¬ {idx} æ¡æµ‹è¯•å‡ºé”™: {e}")
            intent_pred = None
            service_pred = None
            output_text = ""
        
        # è®°å½•ç»“æœ
        result = {
            'idx': idx,
            'input': input_text,
            'intent_true': intent_true,
            'intent_pred': intent_pred,
            'service_true': service_true,
            'service_pred': service_pred,
            'output': output_text,
            'intent_correct': intent_pred == intent_true,
            'service_correct': service_pred == service_true,
            'joint_correct': (intent_pred == intent_true and service_pred == service_true)
        }
        results.append(result)
        
        # æ˜¾ç¤ºè¿›åº¦
        if idx % 10 == 0:
            print(f"å·²æµ‹è¯• {idx}/{len(test_data)} æ¡...")
    
    print("-" * 60)
    print("æµ‹è¯•å®Œæˆï¼\n")
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(results)
    
    # æ˜¾ç¤ºç»“æœ
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(f"æ€»æµ‹è¯•æ ·æœ¬æ•°: {metrics['total']}")
    print(f"\nIntent Type Accuracy: {metrics['intent_accuracy']:.4f} ({metrics['intent_correct']}/{metrics['total']})")
    print(f"Service Type Accuracy: {metrics['service_accuracy']:.4f} ({metrics['service_correct']}/{metrics['total']})")
    print(f"Joint Accuracy: {metrics['joint_accuracy']:.4f} ({metrics['joint_correct']}/{metrics['total']})")
    print("=" * 60)
    
    # æ˜¾ç¤ºé”™è¯¯æ ·æœ¬
    print("\nâŒ é”™è¯¯æ ·æœ¬åˆ†æ:")
    print("-" * 60)
    
    intent_errors = [r for r in results if not r['intent_correct']]
    service_errors = [r for r in results if not r['service_correct']]
    joint_errors = [r for r in results if not r['joint_correct']]
    
    print(f"\nIntenté”™è¯¯ ({len(intent_errors)} ä¸ª):")
    for r in intent_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  è¾“å…¥: {r['input'][:50]}...")
        print(f"  çœŸå®: {r['intent_true']}, é¢„æµ‹: {r['intent_pred']}")
        print(f"  è¾“å‡º: {r['output'][:100]}...")
        print()
    
    print(f"\nServiceé”™è¯¯ ({len(service_errors)} ä¸ª):")
    for r in service_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  è¾“å…¥: {r['input'][:50]}...")
        print(f"  çœŸå®: {r['service_true']}, é¢„æµ‹: {r['service_pred']}")
        print(f"  è¾“å‡º: {r['output'][:100]}...")
        print()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "test_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metrics': metrics,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()


